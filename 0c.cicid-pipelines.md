##prereqs:
assume kubernetes and kubeflow already installed
assume we already installed all the compiler packages in the virtual env in guide "1.pipeline-prereqs.md"

## Continous Integration pipelines
we need 3 environments cor Continous Integration:
* kubernetes master node
* a node with dockercli installed to build images
* a node free that is serving files through http/s


1. build docker image:
2. on master node: compile and place the compiled pipeline version in te directory being served by apache

WARING: DO ON NON K8S HOST, it always messes containerd config
on host with docercli, build and push new docker image
```
git clone https://github.com/DarianHarrison/kf
cd kf/3.kf-components/1.pipelines/
sh docker-install.sh
```

compile and place the compiled pipeline version in te directory being served by apache
```
sh recompile-serve.sh # to build compile and serve image
```

3. 
on ui (get tar.gz file from server):
```
upload from url> http://10.163.168.140/staticfiles/<pipeline-name>.tar.gz
create experiment (name = first-experiment ) > create run > choose experiment (first-experiment) > start
```

on k8s master you can verify
```
k get all -n anonymous
```

4.

had to taint due to webhook error
```
kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io --all
kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io --all
```



DONE !!!



11. (optional) For a More Detailed Description

Pipelines from scratch:
https://www.kubeflow.org/docs/pipelines/sdk/component-development/

The pipeline includes:
	-	A pipeline component is a self-contained set of user code, packaged as a Docker image,
	-	inputs: (parameters) required to run the pipeline and the inputs and outputs of each component.
	-	Outputs: Your component can create outputs that the downstream components can use as inputs. Each output must be a string and the container image must write each output to a separate local text file. For example, if a training component needs to output the path of the trained model, the component writes the path into a local file, such as /output.txt. In the Python class that defines your pipeline (see below) you can specify how to map the content of local files to component outputs.


* 1) **program.py** Write the program that contains your component’s logic. The program must use files and command-line arguments to pass data to and from the component.ent.
```
Write your application code, program.py. For example, write code to transform data or train a model.
```
* 2) **build-image.sh** Containerize the program.
```
Create a Docker container image that packages your program (program.py) and upload the container image to a registry.
```
* 3) **component.yaml** Write a component specification in YAML format that describes the component for the Kubeflow Pipelines system.
```
Write a pipeline function using the Kubeflow Pipelines DSL to define the pipeline and include all the pipeline components.
```
* 4) Use the Kubeflow Pipelines SDK to load your component, use it in a pipeline and run that pipeline.
a) **pipelines.py** 
```
Write a component function using the Kubeflow Pipelines DSL to define your pipeline’s interactions with the component’s Docker container.
```
b) **pipelines.tar.gz** Compile the pipeline to generate a compressed YAML definition of the pipeline. 
```
dsl-compile --py [path/to/python/pipelines.py] --output pipelines.tar.gz
```
* 5) **run-pipelines.py** Test, Use the Kubeflow Pipelines SDK to run the pipeline:
```
client-pipeline.py
```

* **steps to build share your pipeline** 

* create python script with invocation arguments
* test script and arguments
* conteinerize the program
* create component.yaml
* compile the code
* push to shared directory